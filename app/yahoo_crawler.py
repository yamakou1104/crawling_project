import sys
import requests
from bs4 import BeautifulSoup

sys.stdout.reconfigure(line_buffering=True)  # ãƒ­ã‚°ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤º

URL = "https://news.yahoo.co.jp/expert/articles/31a65afbecc42b3780a6761a39f0c511a0f20948"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

print("ğŸŸ¢ ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’é–‹å§‹ã—ã¾ã—ãŸï¼")

try:
    print("ğŸ” Webãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")

    response = requests.get(URL, headers=HEADERS)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾—
        title = soup.title.text if soup.title else "No Title"
        print(f"âœ… ãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        
        # è¨˜äº‹æœ¬æ–‡ã®å–å¾—
        print("ğŸ” è¨˜äº‹æœ¬æ–‡ã‚’æŠ½å‡ºä¸­...")
        
        # è¨˜äº‹ã®æ®µè½ã‚’å–å¾—
        paragraphs = soup.find_all('p')
        article_paragraphs = []
        
        # è¨˜äº‹ã®æœ¬æ–‡ã‚’å«ã‚€å¯èƒ½æ€§ã®ã‚ã‚‹æ®µè½ã‚’æŠ½å‡ºï¼ˆçŸ­ã™ãã‚‹æ®µè½ã¯é™¤å¤–ï¼‰
        for p in paragraphs:
            text = p.text.strip()
            # 50æ–‡å­—ä»¥ä¸Šã®æ®µè½ã‚’è¨˜äº‹æœ¬æ–‡ã¨ã—ã¦æ‰±ã†
            if len(text) > 50:
                article_paragraphs.append(text)
        
        if article_paragraphs:
            print("âœ… è¨˜äº‹æœ¬æ–‡:")
            for i, paragraph in enumerate(article_paragraphs):
                print(f"  æ®µè½ {i+1}: {paragraph}")
        else:
            print("âŒ è¨˜äº‹æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
        # è¨˜äº‹å…¨ä½“ã‚’ä¸€ã¤ã®æ–‡å­—åˆ—ã¨ã—ã¦çµåˆ
        article_body = "\n\n".join(article_paragraphs)
        
        # ç”»åƒURLã®å–å¾—
        print("\nğŸ” ç”»åƒURLã‚’æŠ½å‡ºä¸­...")
        
        # ç”»åƒè¦ç´ ã‚’å–å¾—
        images = soup.find_all('img')
        article_images = []
        
        # è¨˜äº‹ã®ç”»åƒã‚’æŠ½å‡ºï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ç”»åƒã«ç‰¹å¾´çš„ãªURLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤ã‚‚ã®ã‚’é¸æŠï¼‰
        for img in images:
            src = img.get('src', '')
            if src and ('newsatcl-pctr' in src or 'news-pctr' in src):
                article_images.append(src)
        
        # å›³ï¼ˆfigureï¼‰è¦ç´ å†…ã®ç”»åƒã‚‚å–å¾—
        figures = soup.find_all('figure')
        for fig in figures:
            img = fig.find('img')
            if img and img.get('src'):
                src = img.get('src')
                if src not in article_images:  # é‡è¤‡ã‚’é¿ã‘ã‚‹
                    article_images.append(src)
        
        if article_images:
            print("âœ… ç”»åƒURL:")
            for i, img_url in enumerate(article_images):
                print(f"  ç”»åƒ {i+1}: {img_url}")
        else:
            print("âŒ ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    else:
        print(f"âŒ HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")

except Exception as e:
    print(f"âš ï¸ ä¾‹å¤–ç™ºç”Ÿ: {e}")

print("âœ… ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Œäº†ï¼")
